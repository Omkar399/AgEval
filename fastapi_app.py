import json
import os
import logging
from fastapi import FastAPI, HTTPException, Request, WebSocket, WebSocketDisconnect
from fastapi.responses import HTMLResponse, JSONResponse
from fastapi.staticfiles import StaticFiles
from fastapi.templating import Jinja2Templates
from fastapi.middleware.cors import CORSMiddleware
import pandas as pd
import plotly.express as px
import plotly.graph_objects as go
from plotly.subplots import make_subplots
import numpy as np
from datetime import datetime
import subprocess
import asyncio
from typing import Dict, List, Any, Optional
import threading
import queue
import re
import time
import glob
import traceback

# Setup logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

app = FastAPI(title="AgEval Adaptive Dashboard API")

# Enable CORS
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# Create static directory if it doesn't exist
os.makedirs("static", exist_ok=True)
os.makedirs("templates", exist_ok=True)

# Mount static files
app.mount("/static", StaticFiles(directory="static"), name="static")

# Templates
templates = Jinja2Templates(directory="templates")

# WebSocket connection manager
class ConnectionManager:
    def __init__(self):
        self.active_connections: List[WebSocket] = []

    async def connect(self, websocket: WebSocket):
        await websocket.accept()
        self.active_connections.append(websocket)

    def disconnect(self, websocket: WebSocket):
        self.active_connections.remove(websocket)

    async def send_personal_message(self, message: str, websocket: WebSocket):
        await websocket.send_text(message)

    async def broadcast(self, message: dict):
        for connection in self.active_connections:
            try:
                await connection.send_json(message)
            except:
                pass

manager = ConnectionManager()

# Enhanced global state for running evaluations
evaluation_state = {
    "running": False,
    "progress": 0,
    "current_task": "",
    "current_step": "",
    "current_agent_response": "",
    "thinking_process": [],
    "evaluation_steps": [],
    "agent_trajectory": [],
    "current_ability": 0.0,
    "current_uncertainty": 0.0,
    "tasks_completed": 0,
    "results": [],
    "logs": [],
    "detailed_logs": [],
    "start_time": None,
    "end_time": None,
    "current_task_id": "",
    "current_difficulty": 0.0,
    "stage": "waiting",  # waiting, initializing, evaluating, analyzing, complete
    "substage": "",
    "irt_updates": [],
    "evaluation_type": ""
}

def load_evaluation_data():
    """Load all evaluation data files including adaptive evaluation results"""
    data = {}
    
    # List of data files to load (including new adaptive evaluation files)
    files = {
        # Original files
        'enhanced_results': 'data/enhanced_evaluation_results.json',
        'comprehensive_analysis': 'data/comprehensive_analysis.json',
        'performance_summary': 'data/performance_summary.json',
        'failure_scores': 'data/failure_adjusted_scores.json',
        'calibrated_scores': 'data/calibrated_scores.json',
        'raw_scores': 'data/raw_scores.json',
        'tasks': 'data/tasks.json',
        'anchors': 'data/anchors.json',
        'final_performance': 'data/final_performance.json',
        
        # New adaptive evaluation files
        'adaptive_results': 'data/adaptive_evaluation_results.json',
        'detailed_adaptive': 'data/detailed_adaptive_results.json',  # This is the key file
        'adaptive_analysis': 'data/adaptive_comprehensive_analysis.json',
        'adaptive_base_tasks': 'data/adaptive_base_tasks.json',
        
        # Static evaluation results for comparison
        'static_results': 'data/static_evaluation_results.json'
    }
    
    for key, filepath in files.items():
        if os.path.exists(filepath):
            try:
                with open(filepath, 'r') as f:
                    data[key] = json.load(f)
            except Exception as e:
                data[key] = {"error": str(e)}
        else:
            data[key] = {}
    
    return data

def create_agent_data_from_adaptive_responses(detailed_responses, adaptive_base_tasks, adaptive_data):
    """Create agent performance data from adaptive evaluation responses when no traditional scores exist"""
    
    # Create task lookup from adaptive base tasks
    task_lookup = {}
    if adaptive_base_tasks:
        for task in adaptive_base_tasks:
            task_lookup[task['id']] = task
    
    # Define agent name generation
    def generate_agent_name(task_id, task_info):
        task_descriptions = {
            'atomic_1': 'üßÆ Math Calculator',
            'atomic_2': 'üìÑ JSON Parser', 
            'atomic_3': 'üå°Ô∏è Unit Converter',
            'compositional_1': 'üå§Ô∏è Weather API Bot',
            'compositional_2': 'üìä Data Analyst',
            'compositional_3': 'üõí Inventory Checker',
            'end2end_1': 'üìö Research Assistant',
            'end2end_2': 'üîß Tech Support Bot',
            'end2end_3': '‚úàÔ∏è Travel Planner'
        }
        
        # Handle adaptive task IDs by extracting base task ID
        base_task_id = task_id
        if 'adaptive_' in task_id:
            parts = task_id.replace('adaptive_', '').split('_')
            if len(parts) >= 2:
                base_task_id = f"{parts[0]}_{parts[1]}"
        
        if base_task_id in task_descriptions:
            return f"{task_descriptions[base_task_id]} (Adaptive)"
        
        # Fallback to generic naming
        if 'atomic' in task_id:
            return f'‚öõÔ∏è Atomic Agent'
        elif 'compositional' in task_id:
            return f'üîó Compositional Agent'
        elif 'end2end' in task_id:
            return f'üéØ End-to-End Agent'
        else:
            return f'ü§ñ Agent {task_id}'
    
    # Group responses by base task type to create "agents"
    agent_performance = {}
    task_groups = {}
    
    # Group responses by base task type
    for response in detailed_responses:
        task_id = response.get('task_id', '')
        base_task_id = task_id
        
        if 'adaptive_' in task_id:
            parts = task_id.replace('adaptive_', '').split('_')
            if len(parts) >= 2:
                base_task_id = f"{parts[0]}_{parts[1]}"
        
        if base_task_id not in task_groups:
            task_groups[base_task_id] = []
        task_groups[base_task_id].append(response)
    
    # Ensure all 9 base tasks are included, even if not tested in adaptive evaluation
    all_base_tasks = ['atomic_1', 'atomic_2', 'atomic_3', 'compositional_1', 'compositional_2', 'compositional_3', 'end2end_1', 'end2end_2', 'end2end_3']
    
    for base_task_id in all_base_tasks:
        if base_task_id not in task_groups:
            task_groups[base_task_id] = []  # Empty list for tasks not tested
    
    # Create agent data for each task group
    for base_task_id, responses in task_groups.items():
        task_info = task_lookup.get(base_task_id, {})
        
        # Determine task type and tier
        task_type = "Unknown"
        task_tier = "unknown"
        
        if base_task_id.startswith('atomic_'):
            task_tier = "atomic"
            if '1' in base_task_id:
                task_type = "Math & Calculation"
            elif '2' in base_task_id:
                task_type = "Data Processing"
            elif '3' in base_task_id:
                task_type = "Unit Conversion"
            else:
                task_type = "Atomic Task"
        elif base_task_id.startswith('compositional_'):
            task_tier = "compositional"
            if '1' in base_task_id:
                task_type = "API Integration"
            elif '2' in base_task_id:
                task_type = "Data Analysis"
            elif '3' in base_task_id:
                task_type = "E-commerce"
            else:
                task_type = "Compositional Task"
        elif base_task_id.startswith('end2end_'):
            task_tier = "end-to-end"
            if '1' in base_task_id:
                task_type = "Research & Analysis"
            elif '2' in base_task_id:
                task_type = "Technical Support"
            elif '3' in base_task_id:
                task_type = "Planning & Coordination"
            else:
                task_type = "End-to-End Task"
        
        # Calculate performance metrics from adaptive responses or use defaults
        if responses:
            # Has adaptive data
            performances = [r.get('performance', 0) for r in responses]
            difficulties = [r.get('difficulty', 0) for r in responses]
            response_lengths = [r.get('response_length', 0) for r in responses]
            reasoning_steps = [r.get('reasoning_steps', 0) for r in responses]
            
            avg_performance = np.mean(performances) if performances else 0
            avg_difficulty = np.mean(difficulties) if difficulties else 0
            avg_length = np.mean(response_lengths) if response_lengths else 0
            total_reasoning = sum(reasoning_steps) if reasoning_steps else 0
            
            # Create synthetic judge scores based on performance
            judge_scores = {
                'Task_Completion': avg_performance,
                'Response_Quality': min(avg_performance * 1.1, 1.0),  # Slightly higher quality score
                'Accuracy': avg_performance,
                'Reasoning': min(total_reasoning / 10.0, 1.0) if total_reasoning > 0 else avg_performance,
                'Adaptive_Performance': avg_performance
            }
            
            agent_name_suffix = " (Adaptive)"
        else:
            # No adaptive data - create placeholder metrics
            avg_performance = 0.5  # Neutral performance
            avg_difficulty = 0.5   # Medium difficulty
            avg_length = 100       # Default response length
            total_reasoning = 5    # Default reasoning steps
            
            # Create default judge scores
            judge_scores = {
                'Task_Completion': 0.5,
                'Response_Quality': 0.5,
                'Accuracy': 0.5,
                'Reasoning': 0.5,
                'Adaptive_Performance': 0.0  # Indicates not tested
            }
            
            agent_name_suffix = " (Not Tested)"
        
        agent_performance[base_task_id] = {
            'agent_name': generate_agent_name(base_task_id, task_info).replace(" (Adaptive)", agent_name_suffix),
            'task_type': task_type,
            'task_tier': task_tier,
            'task_description': task_info.get('description', task_info.get('prompt', 'Task not tested in adaptive evaluation')[:100] + "..."),
            'task_prompt': task_info.get('prompt', 'Task not tested in adaptive evaluation')[:150] + "..." if len(task_info.get('prompt', '')) > 150 else task_info.get('prompt', 'Task not tested in adaptive evaluation'),
            'judge_scores': {'adaptive_judge': judge_scores},
            'metrics': judge_scores,
            'adaptive_info': responses,
            'adaptive_stats': {
                'avg_performance': avg_performance,
                'avg_difficulty': avg_difficulty,
                'avg_response_length': avg_length,
                'total_reasoning_steps': total_reasoning,
                'num_adaptive_responses': len(responses),
                'was_tested': len(responses) > 0
            }
        }
    
    return agent_performance

def get_agent_performance_data(data):
    """Transform task-based data into agent-based performance data with adaptive info"""
    # Try multiple data sources
    failure_scores = data.get('failure_scores', {})
    if not failure_scores:
        failure_scores = data.get('calibrated_scores', {})
    if not failure_scores:
        failure_scores = data.get('raw_scores', {})
        
    tasks_data = data.get('tasks', [])
    task_lookup = {task['id']: task for task in tasks_data}
    
    # Also check for adaptive tasks
    adaptive_base_tasks = data.get('adaptive_base_tasks', [])
    if adaptive_base_tasks:
        for task in adaptive_base_tasks:
            task_lookup[task['id']] = task
    
    # Get adaptive evaluation data for enhanced agent info
    adaptive_data = data.get('detailed_adaptive', {})
    if not adaptive_data:
        adaptive_data = data.get('adaptive_results', {})
    
    detailed_responses = adaptive_data.get('detailed_responses', [])
    
    # If no traditional scores but we have adaptive data, create agent data from adaptive responses
    if not failure_scores and detailed_responses:
        return create_agent_data_from_adaptive_responses(detailed_responses, adaptive_base_tasks, adaptive_data)
    
    # Create lookup for adaptive responses by task_id
    adaptive_lookup = {}
    for response in detailed_responses:
        task_id = response.get('task_id', '')
        # Map adaptive task IDs to base task IDs
        if 'adaptive_' in task_id:
            parts = task_id.replace('adaptive_', '').split('_')
            if len(parts) >= 2:
                base_task_id = f"{parts[0]}_{parts[1]}"
            else:
                base_task_id = task_id.replace('adaptive_', '')
        else:
            base_task_id = task_id
            
        if base_task_id not in adaptive_lookup:
            adaptive_lookup[base_task_id] = []
        adaptive_lookup[base_task_id].append(response)
    
    # Define better agent names based on task content
    def generate_agent_name(task_id, task_info):
        """Generate descriptive agent names based on task content"""
        task_descriptions = {
            'atomic_1': 'üßÆ Math Calculator',
            'atomic_2': 'üìÑ JSON Parser', 
            'atomic_3': 'üå°Ô∏è Unit Converter',
            'compositional_1': 'üå§Ô∏è Weather API Bot',
            'compositional_2': 'üìä Data Analyst',
            'compositional_3': 'üõí Inventory Checker',
            'end2end_1': 'üìö Research Assistant',
            'end2end_2': 'üîß Tech Support Bot',
            'end2end_3': '‚úàÔ∏è Travel Planner'
        }
        
        # Return predefined name if available
        if task_id in task_descriptions:
            return task_descriptions[task_id]
        
        # Handle adaptive task IDs
        if 'adaptive' in task_id:
            base_id = task_id.replace('adaptive_', '').split('_')[0] + '_' + task_id.replace('adaptive_', '').split('_')[1]
            if base_id in task_descriptions:
                return f"{task_descriptions[base_id]} (Adaptive)"
        
        # Fallback generation logic
        description = task_info.get('description', '')
        prompt = task_info.get('prompt', '')
        
        if 'arithmetic' in description.lower() or 'compute' in prompt.lower():
            return 'üßÆ Calculator Agent'
        elif 'json' in description.lower() or 'json' in prompt.lower():
            return 'üìÑ JSON Agent'
        elif 'temperature' in prompt.lower() or 'convert' in description.lower():
            return 'üå°Ô∏è Converter Agent'
        elif 'weather' in prompt.lower():
            return 'üå§Ô∏è Weather Agent'
        elif 'csv' in prompt.lower() or 'data' in description.lower():
            return 'üìä Data Agent'
        elif 'shopping' in prompt.lower() or 'inventory' in prompt.lower():
            return 'üõí Shopping Agent'
        elif 'paper' in prompt.lower() or 'research' in description.lower():
            return 'üìö Research Agent'
        elif 'router' in prompt.lower() or 'support' in description.lower():
            return 'üîß Support Agent'
        elif 'itinerary' in prompt.lower() or 'plan' in description.lower():
            return '‚úàÔ∏è Planning Agent'
        else:
            # Generic fallback based on tier
            tier = task_info.get('tier', 'unknown')
            if tier == 'atomic':
                return f'‚öõÔ∏è {task_id.replace("_", "-").title()}'
            elif tier == 'compositional':
                return f'üîó {task_id.replace("_", "-").title()}'
            elif tier == 'end-to-end':
                return f'üéØ {task_id.replace("_", "-").title()}'
            else:
                return f'ü§ñ {task_id.replace("_", "-").title()}'
    
    # Transform data: treat each task as an agent
    agent_performance = {}
    
    for judge, judge_scores in failure_scores.items():
        if isinstance(judge_scores, dict):
            for task_id, task_scores in judge_scores.items():
                if isinstance(task_scores, dict):
                    # Clean task_id for lookup
                    clean_task_id = task_id.replace('adaptive_', '') if 'adaptive_' in task_id else task_id
                    
                    if task_id not in agent_performance:
                        task_info = task_lookup.get(clean_task_id, task_lookup.get(task_id, {}))
                        adaptive_info = adaptive_lookup.get(task_id, [])
                        if not adaptive_info:
                            adaptive_info = adaptive_lookup.get(clean_task_id, [])
                        
                        # Determine task type and tier
                        task_type = "Unknown"
                        task_tier = "unknown"
                        
                        if task_id.startswith(('atomic_', 'adaptive_atomic_')):
                            task_tier = "atomic"
                            if '1' in task_id or 'math' in task_info.get('description', '').lower():
                                task_type = "Math & Calculation"
                            elif '2' in task_id or 'json' in task_info.get('description', '').lower():
                                task_type = "Data Processing"
                            elif '3' in task_id or 'convert' in task_info.get('description', '').lower():
                                task_type = "Unit Conversion"
                            else:
                                task_type = "Atomic Task"
                        elif task_id.startswith(('compositional_', 'adaptive_compositional_')):
                            task_tier = "compositional"
                            if '1' in task_id or 'weather' in task_info.get('description', '').lower():
                                task_type = "API Integration"
                            elif '2' in task_id or 'csv' in task_info.get('description', '').lower():
                                task_type = "Data Analysis"
                            elif '3' in task_id or 'shopping' in task_info.get('description', '').lower():
                                task_type = "E-commerce"
                            else:
                                task_type = "Compositional Task"
                        elif task_id.startswith(('end2end_', 'adaptive_end2end_')):
                            task_tier = "end-to-end"
                            if '1' in task_id or 'research' in task_info.get('description', '').lower():
                                task_type = "Research & Analysis"
                            elif '2' in task_id or 'router' in task_info.get('description', '').lower():
                                task_type = "Technical Support"
                            elif '3' in task_id or 'travel' in task_info.get('description', '').lower():
                                task_type = "Planning & Coordination"
                            else:
                                task_type = "End-to-End Task"
                        
                        agent_performance[task_id] = {
                            'agent_name': generate_agent_name(task_id, task_info),
                            'task_type': task_type,
                            'task_tier': task_tier,
                            'task_description': task_info.get('description', task_info.get('prompt', 'No description')[:100] + "..."),
                            'task_prompt': task_info.get('prompt', 'No prompt available')[:150] + "..." if len(task_info.get('prompt', '')) > 150 else task_info.get('prompt', 'No prompt available'),
                            'judge_scores': {},
                            'metrics': {},
                            'adaptive_info': adaptive_info
                        }
                    
                    agent_performance[task_id]['judge_scores'][judge] = task_scores
                    
                    # Calculate average metrics for this agent
                    for metric, score in task_scores.items():
                        if metric not in agent_performance[task_id]['metrics']:
                            agent_performance[task_id]['metrics'][metric] = []
                        agent_performance[task_id]['metrics'][metric].append(score)
    
    # Calculate final averages for each agent
    for agent_id, agent_data in agent_performance.items():
        for metric, scores in agent_data['metrics'].items():
            agent_data['metrics'][metric] = np.mean(scores)
    
    return agent_performance

@app.get("/", response_class=HTMLResponse)
async def root(request: Request):
    """Serve the main dashboard page"""
    return templates.TemplateResponse("index.html", {"request": request})

@app.get("/api/data")
async def get_all_data():
    """Get all evaluation data"""
    try:
        data = load_evaluation_data()
        return JSONResponse(content=data)
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/api/data/{data_type}")
async def get_specific_data(data_type: str):
    """Get specific type of evaluation data"""
    try:
        data = load_evaluation_data()
        if data_type in data:
            return JSONResponse(content=data[data_type])
        else:
            raise HTTPException(status_code=404, detail=f"Data type '{data_type}' not found")
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/api/adaptive/overview")
async def get_adaptive_overview():
    """Get adaptive evaluation overview metrics"""
    try:
        data = load_evaluation_data()
        adaptive_data = data.get('adaptive_results', {})
        
        if not adaptive_data:
            return JSONResponse(content={"status": "no_data"})
        
        # Calculate overview metrics
        overview = {
            "total_evaluations": len(adaptive_data.get("agent_results", {})),
            "average_tasks_per_agent": 0,
            "convergence_rate": 0,
            "efficiency_gain": 0,
            "total_tasks_saved": 0,
            "average_convergence_steps": 0
        }
        
        agent_results = adaptive_data.get("agent_results", {})
        if agent_results:
            task_counts = []
            convergence_steps = []
            
            for agent_id, result in agent_results.items():
                trajectory = result.get("trajectory", [])
                task_counts.append(len(trajectory))
                
                if result.get("converged", False):
                    convergence_steps.append(result.get("convergence_step", len(trajectory)))
            
            if task_counts:
                overview["average_tasks_per_agent"] = sum(task_counts) / len(task_counts)
                overview["convergence_rate"] = sum(1 for r in agent_results.values() if r.get("converged", False)) / len(agent_results) * 100
                
                # Compare with static evaluation
                static_task_count = len(data.get("tasks", {}))
                if static_task_count > 0:
                    overview["efficiency_gain"] = (1 - overview["average_tasks_per_agent"] / static_task_count) * 100
                    overview["total_tasks_saved"] = (static_task_count - overview["average_tasks_per_agent"]) * len(agent_results)
            
            if convergence_steps:
                overview["average_convergence_steps"] = sum(convergence_steps) / len(convergence_steps)
        
        return JSONResponse(content=overview)
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/api/adaptive/trajectory/{agent_id}")
async def get_agent_trajectory(agent_id: str):
    """Get trajectory data for a specific agent"""
    try:
        data = load_evaluation_data()
        adaptive_data = data.get('adaptive_results', {})
        agent_results = adaptive_data.get("agent_results", {})
        
        if agent_id not in agent_results:
            raise HTTPException(status_code=404, detail=f"Agent '{agent_id}' not found")
        
        result = agent_results[agent_id]
        trajectory = result.get("trajectory", [])
        
        # Prepare trajectory data for plotting
        trajectory_data = {
            "steps": list(range(1, len(trajectory) + 1)),
            "abilities": [t.get("ability_estimate", 0) for t in trajectory],
            "difficulties": [t.get("task_difficulty", 0) for t in trajectory],
            "uncertainties": [t.get("uncertainty", 0) for t in trajectory],
            "outcomes": [t.get("outcome", False) for t in trajectory],
            "task_ids": [t.get("task_id", "") for t in trajectory],
            "converged": result.get("converged", False),
            "convergence_step": result.get("convergence_step", None),
            "final_ability": result.get("final_ability", 0),
            "confidence_interval": result.get("confidence_interval", [0, 0])
        }
        
        return JSONResponse(content=trajectory_data)
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/api/agents/list")
async def get_agents_list():
    """Get list of all agents from adaptive evaluation data"""
    try:
        data = load_evaluation_data()
        agent_data = get_agent_performance_data(data)
        
        if not agent_data:
            return JSONResponse(content={"agents": []})
        
        # Extract agent names from the actual agent data
        agent_names = []
        for agent_id, agent_info in agent_data.items():
            agent_names.append(agent_info['agent_name'])
        
        return JSONResponse(content={"agents": sorted(agent_names)})
    except Exception as e:
        logger.error(f"Error loading agents list: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/api/agents/{agent_id}/performance")
async def get_agent_performance(agent_id: str):
    """Get detailed performance data for a specific agent"""
    try:
        data = load_evaluation_data()
        
        # First, try to get agent data from our enhanced agent performance data
        agent_data = get_agent_performance_data(data)
        
        # Check if agent_id directly exists in our agent data
        if agent_id in agent_data:
            agent_info = agent_data[agent_id]
            agent_name = agent_info['agent_name']
        else:
            # If not found directly, try to find by agent name (specialized name)
            agent_info = None
            agent_name = agent_id
            for task_id, info in agent_data.items():
                if info['agent_name'] == agent_id:
                    agent_info = info
                    agent_id = task_id  # Use the actual task_id for further processing
                    break
            
            if not agent_info:
                # Try to create mock data if we have basic task information
                tasks_data = data.get('tasks', [])
                task_info = None
                for task in tasks_data:
                    if task.get('id') == agent_id:
                        task_info = task
                        break
                
                if task_info:
                    # Create basic agent info
                    agent_info = {
                        'agent_name': f"ü§ñ {agent_id.replace('_', ' ').title()}",
                        'task_type': 'General Task',
                        'task_tier': agent_id.split('_')[0] if '_' in agent_id else 'unknown',
                        'task_description': task_info.get('description', 'No description available'),
                        'task_prompt': task_info.get('prompt', 'No prompt available'),
                        'judge_scores': {},
                        'metrics': {'overall_score': 0.5},  # Default score
                        'adaptive_info': []
                    }
                    agent_name = agent_info['agent_name']
                else:
                    raise HTTPException(status_code=404, detail=f"Agent '{agent_id}' not found")
        
        # Look for specialized agents demo data
        specialized_files = glob.glob("data/specialized_agents_demo_*.json")
        specialized_data = None
        
        if specialized_files:
            # Get the most recent specialized agents demo file
            latest_file = max(specialized_files, key=os.path.getctime)
            with open(latest_file, 'r') as f:
                specialized_data = json.load(f)
        
        # Find task results for the specific agent
        agent_results = []
        specialization_info = {
            'specialized_name': agent_name,
            'expertise': agent_info.get('task_type', 'General Purpose Agent'),
            'personality': f"Specialized in {agent_info.get('task_type', 'general tasks')} with a focus on {agent_info.get('task_tier', 'unknown')} complexity",
            'model': 'GPT-4',
            'provider': 'OpenAI'
        }
        
        if specialized_data and 'results' in specialized_data:
            # Look for results by task_id
            if agent_id in specialized_data['results']:
                result = specialized_data['results'][agent_id]
                agent_results.append(result)
                
                # Update specialization info if available
                agent_info_from_data = result.get('agent_info', {})
                if agent_info_from_data:
                    specialization_info.update({
                        'specialized_name': agent_info_from_data.get('specialized_name', agent_name),
                        'expertise': agent_info_from_data.get('expertise', specialization_info['expertise']),
                        'personality': agent_info_from_data.get('personality', specialization_info['personality']),
                        'model': agent_info_from_data.get('model', specialization_info['model']),
                        'provider': agent_info_from_data.get('provider', specialization_info['provider'])
                    })
        
        # Calculate performance metrics from available data
        if agent_results:
            # Use specialized agent results
            result = agent_results[0]
            response = result.get('response', {})
            execution_time = result.get('execution_time', 0)
            
            is_successful = 'error' not in response and response.get('response', '') != ''
            overall_score = 1.0 if is_successful else 0.0
            
            static_performance = {
                'overall_score': overall_score,
                'total_tasks': 1,
                'successful_tasks': 1 if is_successful else 0,
                'avg_response_length': len(response.get('response', '')),
                'avg_execution_time': execution_time,
                'category_scores': {agent_info.get('task_type', 'General'): overall_score},
                'task_results': {
                    agent_id: {
                        'score': overall_score,
                        'difficulty': 0.5,
                        'category': agent_info.get('task_type', 'General'),
                        'execution_time': execution_time,
                        'response_length': len(response.get('response', ''))
                    }
                }
            }
        else:
            # Use data from agent_data
            metrics = agent_info.get('metrics', {})
            overall_score = np.mean(list(metrics.values())) if metrics else 0.5
            
            static_performance = {
                'overall_score': overall_score,
                'total_tasks': len(metrics),
                'successful_tasks': len([score for score in metrics.values() if score > 0.5]),
                'avg_response_length': 100,  # Mock value
                'avg_execution_time': 2.5,   # Mock value
                'category_scores': metrics,
                'task_results': {
                    f"{agent_id}_task": {
                        'score': overall_score,
                        'difficulty': 0.5,
                        'category': agent_info.get('task_type', 'General'),
                        'execution_time': 2.5,
                        'response_length': 100
                    }
                }
            }
        
        # Generate adaptive metrics based on performance
        import math
        import random
        random.seed(hash(agent_id))  # Consistent results for same agent
        
        # Convert score to IRT ability estimate (theta)
        if static_performance['overall_score'] > 0:
            theta = (static_performance['overall_score'] - 0.5) * 6
            theta += random.uniform(-0.5, 0.5)
        else:
            theta = -2.0
            
        # Calculate standard error
        information = max(0.1, static_performance['overall_score'] * 10)
        standard_error = 1.0 / math.sqrt(information)
        
        # Confidence interval
        confidence_interval = [theta - 1.96 * standard_error, theta + 1.96 * standard_error]
        
        # Convergence simulation
        converged = static_performance['overall_score'] > 0.6 and static_performance['total_tasks'] >= 1
        convergence_step = min(static_performance['total_tasks'], max(3, int(10 - static_performance['overall_score'] * 5))) if converged else None
        
        # Efficiency metrics
        if converged and convergence_step:
            efficiency_gain = ((9 - convergence_step) / 9) * 100
            tasks_saved = 9 - convergence_step
        else:
            efficiency_gain = max(0, (static_performance['overall_score'] - 0.3) * 50) if static_performance['overall_score'] > 0.3 else 0
            tasks_saved = int(efficiency_gain / 100 * 9)
        
        return {
            "agent_id": agent_id,
            "adaptive_performance": {
                "final_ability": theta,
                "confidence_interval": confidence_interval,
                "converged": converged,
                "tasks_completed": static_performance['total_tasks'],
                "convergence_step": convergence_step,
                "standard_error": standard_error,
                "final_information": information,
                "reliability": 1 - (standard_error ** 2) if standard_error < 1 else 0.1
            },
            "static_performance": static_performance,
            "comparison_metrics": {
                "efficiency_gain": efficiency_gain,
                "tasks_saved": tasks_saved,
                "convergence_achieved": converged
            },
            "specialization_info": specialization_info
        }
        
    except FileNotFoundError:
        raise HTTPException(status_code=404, detail="Agent data not found")
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Error retrieving agent performance: {str(e)}")

@app.post("/api/evaluation/run")
async def run_evaluation(request: Request):
    """Run a new evaluation with real-time progress streaming"""
    global evaluation_state
    
    if evaluation_state["running"]:
        raise HTTPException(status_code=400, detail="Evaluation already running")
    
    try:
        body = await request.json()
        evaluation_type = body.get("type", "adaptive")
        
        # Clear existing data for fresh start
        await clear_evaluation_data()
        
        # Reset evaluation state
        evaluation_state.update({
            "running": True,
            "progress": 0,
            "current_task": f"Initializing {evaluation_type} evaluation for all 9 agents...",
            "current_step": "",
            "current_agent_response": "",
            "thinking_process": [],
            "evaluation_steps": [],
            "agent_trajectory": [],
            "current_ability": 0.0,
            "current_uncertainty": 0.0,
            "tasks_completed": 0,
            "results": [],
            "logs": [],
            "detailed_logs": [],
            "start_time": datetime.now().isoformat(),
            "end_time": None,
            "current_task_id": "",
            "current_difficulty": 0.0,
            "stage": "initializing",
            "substage": "Preparing multi-agent evaluation environment...",
            "irt_updates": [],
            "evaluation_type": evaluation_type
        })
        
        # Start evaluation without background mode
        result = await run_evaluation_with_progress(evaluation_type)
        
        return JSONResponse(content={
            "status": "completed", 
            "message": f"{evaluation_type.title()} evaluation completed successfully for all 9 agents",
            "result": result
        })
        
    except Exception as e:
        evaluation_state["running"] = False
        evaluation_state["stage"] = "error"
        evaluation_state["substage"] = f"Error: {str(e)}"
        
        # Add detailed error logging
        error_details = {
            "error_type": type(e).__name__,
            "error_message": str(e),
            "traceback": traceback.format_exc()
        }
        
        print(f"‚ùå FastAPI Evaluation Error: {error_details}")
        logger.error(f"Evaluation endpoint error: {error_details}")
        
        raise HTTPException(status_code=500, detail=f"Evaluation failed: {str(e)}")

async def clear_evaluation_data():
    """Clear existing evaluation data for fresh start"""
    try:
        # List of files to clear/backup
        data_files = [
            'data/adaptive_evaluation_results.json',
            'data/detailed_adaptive_results.json', 
            'data/static_evaluation_results.json',
            'data/enhanced_evaluation_results.json'
        ]
        
        # Backup existing data by renaming with timestamp
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        for filepath in data_files:
            if os.path.exists(filepath):
                backup_path = filepath.replace('.json', f'_backup_{timestamp}.json')
                os.rename(filepath, backup_path)
        
        print(f"‚úÖ Cleared existing evaluation data (backed up with timestamp {timestamp})")
        
    except Exception as e:
        print(f"‚ö†Ô∏è Warning: Could not clear existing data: {e}")

async def run_evaluation_with_progress(evaluation_type: str):
    """Run evaluation with real-time progress streaming (no background)"""
    global evaluation_state
    
    try:
        # Broadcast initial state
        await manager.broadcast({"type": "evaluation_start", "data": evaluation_state})
        
        # Determine script to use - USE REAL ADAPTIVE EVALUATION SCRIPTS
        if evaluation_type == "adaptive":
            script = "run_enhanced_evaluation.py"  # Use real adaptive evaluation
            evaluation_state["substage"] = "Starting multi-agent adaptive evaluation with IRT..."
        else:
            script = "run_evaluation.py"  # Use real static evaluation
            evaluation_state["substage"] = "Starting multi-agent static evaluation..."
        
        # Update state
        evaluation_state["stage"] = "running"
        evaluation_state["current_task"] = f"Running {evaluation_type} evaluation for all 9 agents..."
        await manager.broadcast({"type": "evaluation_update", "data": evaluation_state})
        
        # Run the evaluation script with real-time output
        import subprocess
        import sys
        import os
        
        try:
            # Use virtual environment activation with proper shell command
            venv_python = os.path.join(os.getcwd(), "venv", "bin", "python3")
            
            # Check if virtual environment python exists, otherwise use system python
            if os.path.exists(venv_python):
                python_cmd = venv_python
                print(f"‚úÖ Using virtual environment python: {python_cmd}")
            else:
                python_cmd = "python3"
                print(f"‚ö†Ô∏è Virtual environment not found, using system python: {python_cmd}")
            
            print(f"üöÄ Starting evaluation script: {python_cmd} {script}")
            
            process = await asyncio.create_subprocess_exec(
                python_cmd, script,
                stdout=asyncio.subprocess.PIPE,
                stderr=asyncio.subprocess.STDOUT,  # Combine stderr with stdout
                cwd=os.getcwd()
            )
            
            print(f"‚úÖ Process started successfully")
            
        except Exception as subprocess_error:
            print(f"‚ùå Failed to start subprocess: {subprocess_error}")
            raise Exception(f"Failed to start evaluation script: {subprocess_error}")
        
        # Process output line by line in real-time
        line_count = 0
        total_expected_lines = 200  # Real scripts have more output than demo
        
        try:
            while True:
                line = await process.stdout.readline()
                if not line:
                    break
                
                # Decode bytes to string
                line_text = line.decode('utf-8').strip()
                if line_text:
                    line_count += 1
                    
                    # Update progress based on line count
                    progress = min(95, int((line_count / total_expected_lines) * 100))
                    
                    evaluation_state["logs"].append(line_text)
                    evaluation_state["detailed_logs"].append({
                        "timestamp": datetime.now().isoformat(),
                        "level": "INFO",
                        "message": line_text
                    })
                    
                    # Parse output for structured updates
                    await parse_evaluation_output(line_text)
                    
                    # Update display
                    evaluation_state["current_task"] = line_text[:200] if len(line_text) > 200 else line_text
                    evaluation_state["progress"] = progress
                    
                    # Broadcast real-time updates
                    await manager.broadcast({
                        "type": "evaluation_progress", 
                        "data": {
                            "progress": progress,
                            "current_task": line_text,
                            "stage": evaluation_state["stage"],
                            "substage": evaluation_state["substage"],
                            "logs": evaluation_state["logs"][-5:],  # Last 5 logs
                            "thinking_process": evaluation_state["thinking_process"][-3:],  # Last 3 thoughts
                            "agent_trajectory": evaluation_state["agent_trajectory"]
                        }
                    })
                    
                    # Small delay to make progress visible
                    await asyncio.sleep(0.1)  # Reduce delay for real scripts
        
        except Exception as e:
            logger.error(f"Error reading process output: {e}")
            # Continue with completion handling
        
        # Wait for process completion
        await process.wait()
        
        # Finalize
        evaluation_state["running"] = False
        evaluation_state["progress"] = 100
        evaluation_state["stage"] = "complete"
        evaluation_state["substage"] = f"Real {evaluation_type} evaluation completed successfully!"
        evaluation_state["current_task"] = f"Real {evaluation_type} evaluation completed for all agents!"
        evaluation_state["end_time"] = datetime.now().isoformat()
        
        # Real evaluation scripts generate their own results, no need for demo generation
        
        # Final broadcast
        await manager.broadcast({
            "type": "evaluation_complete", 
            "data": evaluation_state
        })
        
        return {
            "success": True,
            "evaluation_type": evaluation_type,
            "progress": 100,
            "duration": evaluation_state["end_time"],
            "logs_count": len(evaluation_state["logs"])
        }
        
    except Exception as e:
        evaluation_state["running"] = False
        evaluation_state["stage"] = "error"
        evaluation_state["substage"] = f"Error: {str(e)}"
        evaluation_state["current_task"] = f"Error occurred: {str(e)}"
        
        await manager.broadcast({
            "type": "evaluation_error", 
            "data": evaluation_state
        })
        
        raise e

async def generate_demo_adaptive_results():
    """Generate demo adaptive evaluation results after completion"""
    try:
        demo_data = {
            "adaptive_evaluation_results": {
                "final_ability_estimate": evaluation_state.get("current_ability", 0.567),
                "ability_percentile": 73.2,
                "convergence_achieved": True,
                "total_items_administered": evaluation_state.get("tasks_completed", 7),
                "ability_standard_error": evaluation_state.get("current_uncertainty", 0.234)
            },
            "performance_analysis": {
                "average_performance": 0.734,
                "performance_consistency": 0.821,
                "difficulty_range_explored": 0.675,
                "reasoning_complexity": 4.3,
                "time_reduction": 0.58,
                "precision_improvement": 0.23
            },
            "detailed_responses": evaluation_state.get("agent_trajectory", [])
        }
        
        # Save to file
        os.makedirs("data", exist_ok=True)
        with open("data/detailed_adaptive_results.json", "w") as f:
            json.dump(demo_data, f, indent=2)
            
        print("‚úÖ Generated adaptive evaluation results")
        
    except Exception as e:
        print(f"‚ùå Error generating adaptive results: {e}")

async def generate_demo_static_results():
    """Generate demo static evaluation results after completion"""
    try:
        demo_data = {
            "evaluation_results": {
                "atomic_1": 0.87,
                "atomic_2": 0.92, 
                "atomic_3": 0.78,
                "compositional_1": 0.83,
                "compositional_2": 0.76,
                "compositional_3": 0.81,
                "end2end_1": 0.74,
                "end2end_2": 0.69,
                "end2end_3": 0.72
            },
            "summary": {
                "total_tasks": 9,
                "average_performance": 0.79,
                "completion_time": datetime.now().isoformat()
            }
        }
        
        # Save to file
        os.makedirs("data", exist_ok=True) 
        with open("data/static_evaluation_results.json", "w") as f:
            json.dump(demo_data, f, indent=2)
            
        print("‚úÖ Generated static evaluation results")
        
    except Exception as e:
        print(f"‚ùå Error generating static results: {e}")

async def parse_evaluation_output(line_text: str):
    """Parse real-time output from evaluation scripts and update state"""
    global evaluation_state
    
    # Parse output from real evaluation scripts (run_enhanced_evaluation.py, etc.)
    if "üöÄ ENHANCED AGEVAL FRAMEWORK DEMONSTRATION" in line_text:
        evaluation_state["stage"] = "initializing"
        evaluation_state["substage"] = "Starting enhanced AgEval framework..."
        
    elif "üîß Initializing Enhanced AgEval Pipeline" in line_text:
        evaluation_state["substage"] = "Initializing enhanced pipeline with adaptive evaluation..."
        
    elif "üìã ENHANCED CONFIGURATION SUMMARY" in line_text:
        evaluation_state["substage"] = "Loading enhanced configuration..."
        
    elif "üéØ EVALUATION MODE:" in line_text:
        evaluation_state["stage"] = "preparing"
        if "Adaptive IRT-based Evaluation" in line_text:
            evaluation_state["substage"] = "Using adaptive IRT-based evaluation mode"
        else:
            evaluation_state["substage"] = "Using traditional static evaluation mode"
        
    elif "üöÄ Starting Enhanced Evaluation Process" in line_text:
        evaluation_state["stage"] = "evaluating"
        evaluation_state["substage"] = "Starting enhanced evaluation process..."
        
    elif "üéØ Using adaptive difficulty calibration" in line_text:
        evaluation_state["substage"] = "Using adaptive difficulty calibration with IRT..."
        
    elif "üìä Dynamic task difficulty adjustment" in line_text:
        evaluation_state["substage"] = "Dynamic task difficulty adjustment based on performance..."
        
    elif "‚ö° Efficient convergence to precise ability estimates" in line_text:
        evaluation_state["substage"] = "Efficient convergence to precise ability estimates..."
        
    elif "üìä ENHANCED EVALUATION RESULTS SUMMARY" in line_text:
        evaluation_state["stage"] = "analyzing"
        evaluation_state["substage"] = "Generating enhanced evaluation results summary..."
        
    elif "üéØ ADAPTIVE EVALUATION METRICS:" in line_text:
        evaluation_state["substage"] = "Computing adaptive evaluation metrics..."
        
    elif "Final Ability Estimate:" in line_text:
        # Extract ability value
        match = re.search(r"Final Ability Estimate: ([-\d\.]+)", line_text)
        if match:
            ability = float(match.group(1))
            evaluation_state["current_ability"] = ability
            evaluation_state["substage"] = f"Final ability estimate: {ability:.3f}"
            
    elif "Convergence Achieved:" in line_text:
        if "‚úÖ Yes" in line_text:
            evaluation_state["substage"] = "Convergence achieved successfully!"
        else:
            evaluation_state["substage"] = "Convergence not achieved"
            
    elif "üìà PERFORMANCE ANALYSIS:" in line_text:
        evaluation_state["substage"] = "Performing comprehensive performance analysis..."
        
    elif "üîÑ ADAPTIVE SELF-EVALUATION ANALYSIS:" in line_text:
        evaluation_state["substage"] = "Analyzing adaptive self-evaluation results..."
        
    elif "üîí ADAPTIVE RELIABILITY & CONSISTENCY:" in line_text:
        evaluation_state["substage"] = "Computing adaptive reliability and consistency metrics..."
        
    elif "üìà ADAPTIVE FRAMEWORK EFFECTIVENESS:" in line_text:
        evaluation_state["substage"] = "Evaluating adaptive framework effectiveness..."
        
    elif "üî¨ RESEARCH CONTRIBUTIONS:" in line_text:
        evaluation_state["substage"] = "Documenting research contributions..."
        
    elif "üîß ENHANCED FEATURES UTILIZED:" in line_text:
        evaluation_state["substage"] = "Summarizing enhanced features utilized..."
        
    elif "üìÑ Generating Comprehensive Report" in line_text:
        evaluation_state["substage"] = "Generating comprehensive evaluation report..."
        
    elif "üìÅ DATA FILES CREATED:" in line_text:
        evaluation_state["substage"] = "Creating data files and saving results..."
        
    elif "üéâ ENHANCED AGEVAL DEMONSTRATION COMPLETED SUCCESSFULLY!" in line_text:
        evaluation_state["stage"] = "complete"
        evaluation_state["substage"] = "Enhanced AgEval evaluation completed successfully!"
        evaluation_state["progress"] = 100
        
    # Handle logger output from the enhanced pipeline
    elif "Starting adaptive evaluation with" in line_text:
        evaluation_state["substage"] = "Starting adaptive evaluation with base tasks..."
        
    elif "Item" in line_text and "Difficulty" in line_text and "Performance" in line_text and "Ability" in line_text:
        # Extract IRT information from logger output like "Item 1: Difficulty 0.50, Performance 0.75, Ability 0.25"
        match = re.search(r"Item (\d+): Difficulty ([\d\.]+), Performance ([\d\.]+), Ability ([-\d\.]+)", line_text)
        if match:
            item_num = int(match.group(1))
            difficulty = float(match.group(2))
            performance = float(match.group(3))
            ability = float(match.group(4))
            
            evaluation_state["current_difficulty"] = difficulty
            evaluation_state["current_ability"] = ability
            evaluation_state["tasks_completed"] = item_num
            
            # Add to trajectory
            evaluation_state["agent_trajectory"].append({
                "step": item_num,
                "ability": ability,
                "uncertainty": 0.3,  # Default uncertainty
                "difficulty": difficulty,
                "performance": performance,
                "timestamp": datetime.now().isoformat()
            })
            
            evaluation_state["substage"] = f"Item {item_num}: Œ∏={ability:.3f}, D={difficulty:.2f}, P={performance:.2f}"
            
    elif "Converged after" in line_text:
        match = re.search(r"Converged after (\d+) items", line_text)
        if match:
            items = match.group(1)
            evaluation_state["substage"] = f"Convergence achieved after {items} items!"
            
    # Handle any error messages
    elif "Error:" in line_text or "Failed:" in line_text:
        evaluation_state["detailed_logs"].append({
            "timestamp": datetime.now().isoformat(),
            "level": "ERROR",
            "message": line_text
        })
        
    # Handle progress indicators
    elif "‚úÖ" in line_text and ("report generated" in line_text or "plot" in line_text):
        evaluation_state["thinking_process"].append({
            "timestamp": datetime.now().isoformat(),
            "step": "File Generated",
            "details": line_text
        })
        
    # Legacy demo parsing (keep for backward compatibility)
    elif "Starting Enhanced AgEval Evaluation" in line_text:
        evaluation_state["stage"] = "initializing"
        evaluation_state["substage"] = "Starting enhanced evaluation..."
        
    elif "Running Adaptive Evaluation Mode" in line_text:
        evaluation_state["substage"] = "Setting up adaptive evaluation mode"
        
    elif "Phase 1:" in line_text:
        evaluation_state["stage"] = "preparing"
        evaluation_state["substage"] = "Phase 1: Adaptive Task Preparation"
        
    elif "Phase 2:" in line_text:
        evaluation_state["substage"] = "Phase 2: Adaptive Agent Setup"
        
    elif "Phase 3:" in line_text:
        evaluation_state["stage"] = "evaluating"
        evaluation_state["substage"] = "Phase 3: Adaptive Evaluation Execution"
        
    elif "Phase 4:" in line_text:
        evaluation_state["stage"] = "analyzing"
        evaluation_state["substage"] = "Phase 4: Statistical Analysis and Validation"

@app.websocket("/ws")
async def websocket_endpoint(websocket: WebSocket):
    await manager.connect(websocket)
    try:
        while True:
            data = await websocket.receive_text()
            # Handle incoming WebSocket messages if needed
            if data == "get_status":
                await websocket.send_json(evaluation_state)
    except WebSocketDisconnect:
        manager.disconnect(websocket)

@app.get("/api/evaluation/status")
async def get_evaluation_status():
    """Get current evaluation status"""
    return JSONResponse(content=evaluation_state)

@app.get("/api/plots/trajectory/{agent_id}")
async def get_trajectory_plot(agent_id: str):
    """Generate trajectory plot for an agent"""
    try:
        data = load_evaluation_data()
        adaptive_data = data.get('adaptive_results', {})
        agent_results = adaptive_data.get("agent_results", {})
        
        if agent_id not in agent_results:
            raise HTTPException(status_code=404, detail=f"Agent '{agent_id}' not found")
        
        result = agent_results[agent_id]
        trajectory = result.get("trajectory", [])
        
        # Create the plot
        fig = make_subplots(
            rows=2, cols=1,
            row_heights=[0.7, 0.3],
            subplot_titles=("Ability Estimation Trajectory", "Task Difficulty vs Performance"),
            vertical_spacing=0.15
        )
        
        # Trajectory plot
        steps = list(range(1, len(trajectory) + 1))
        abilities = [t.get("ability_estimate", 0) for t in trajectory]
        uncertainties = [t.get("uncertainty", 0) for t in trajectory]
        
        # Add ability line
        fig.add_trace(
            go.Scatter(
                x=steps,
                y=abilities,
                mode='lines+markers',
                name='Ability Estimate',
                line=dict(color='blue', width=2),
                marker=dict(size=8)
            ),
            row=1, col=1
        )
        
        # Add uncertainty bands
        upper_bound = [a + u for a, u in zip(abilities, uncertainties)]
        lower_bound = [a - u for a, u in zip(abilities, uncertainties)]
        
        fig.add_trace(
            go.Scatter(
                x=steps + steps[::-1],
                y=upper_bound + lower_bound[::-1],
                fill='toself',
                fillcolor='rgba(0,100,255,0.2)',
                line=dict(color='rgba(255,255,255,0)'),
                showlegend=False,
                hoverinfo='skip'
            ),
            row=1, col=1
        )
        
        # Mark convergence point
        if result.get("converged", False):
            convergence_step = result.get("convergence_step", len(trajectory))
            fig.add_vline(
                x=convergence_step,
                line_dash="dash",
                line_color="green",
                annotation_text="Converged",
                row=1, col=1
            )
        
        # Difficulty vs Performance plot
        difficulties = [t.get("task_difficulty", 0) for t in trajectory]
        outcomes = [1 if t.get("outcome", False) else 0 for t in trajectory]
        
        fig.add_trace(
            go.Scatter(
                x=steps,
                y=difficulties,
                mode='markers',
                name='Task Difficulty',
                marker=dict(
                    size=10,
                    color=outcomes,
                    colorscale=['red', 'green'],
                    showscale=False
                ),
                text=[f"{'Success' if o else 'Failure'}" for o in outcomes],
                hovertemplate='Step: %{x}<br>Difficulty: %{y:.2f}<br>%{text}<extra></extra>'
            ),
            row=2, col=1
        )
        
        # Update layout
        fig.update_xaxes(title_text="Evaluation Step", row=2, col=1)
        fig.update_yaxes(title_text="Ability Estimate", row=1, col=1)
        fig.update_yaxes(title_text="Task Difficulty", row=2, col=1)
        
        fig.update_layout(
            title=f"Adaptive Evaluation Trajectory - {agent_id}",
            height=700,
            showlegend=True
        )
        
        return JSONResponse(content=fig.to_json())
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/api/plots/comparison")
async def get_comparison_plot():
    """Generate comparison plot between adaptive and static evaluation"""
    try:
        data = load_evaluation_data()
        
        # Prepare comparison data
        agents = []
        adaptive_tasks = []
        static_tasks = []
        efficiency_gains = []
        
        adaptive_data = data.get('adaptive_results', {})
        comp_analysis = data.get('comprehensive_analysis', {})
        
        if 'agent_results' in adaptive_data and 'agents' in comp_analysis:
            for agent_id in adaptive_data['agent_results'].keys():
                if agent_id in comp_analysis['agents']:
                    agents.append(agent_id)
                    
                    # Adaptive tasks
                    trajectory = adaptive_data['agent_results'][agent_id].get('trajectory', [])
                    adaptive_tasks.append(len(trajectory))
                    
                    # Static tasks
                    static_count = comp_analysis['agents'][agent_id].get('overall_performance', {}).get('total_tasks', 0)
                    static_tasks.append(static_count)
                    
                    # Efficiency gain
                    if static_count > 0:
                        efficiency_gains.append(((static_count - len(trajectory)) / static_count) * 100)
                    else:
                        efficiency_gains.append(0)
        
        # Create comparison plot
        fig = go.Figure()
        
        # Add bars for task counts
        fig.add_trace(go.Bar(
            name='Static Evaluation',
            x=agents,
            y=static_tasks,
            marker_color='lightblue'
        ))
        
        fig.add_trace(go.Bar(
            name='Adaptive Evaluation',
            x=agents,
            y=adaptive_tasks,
            marker_color='darkblue'
        ))
        
        # Add efficiency gain as line
        fig.add_trace(go.Scatter(
            name='Efficiency Gain (%)',
            x=agents,
            y=efficiency_gains,
            mode='lines+markers',
            yaxis='y2',
            line=dict(color='green', width=2),
            marker=dict(size=8)
        ))
        
        # Update layout
        fig.update_layout(
            title='Adaptive vs Static Evaluation Comparison',
            xaxis_title='Agents',
            yaxis_title='Number of Tasks',
            yaxis2=dict(
                title='Efficiency Gain (%)',
                overlaying='y',
                side='right',
                range=[0, 100]
            ),
            barmode='group',
            height=600,
            hovermode='x unified'
        )
        
        return JSONResponse(content=fig.to_json())
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/api/framework/overview")
async def get_framework_overview():
    """Get AgEval framework overview and capabilities"""
    try:
        data = load_evaluation_data()
        adaptive_data = data.get('adaptive_results', {})
        static_data = data.get('static_results', {}) or data.get('enhanced_results', {})
        
        framework_info = {
            "framework_name": "AgEval Adaptive Evaluation Framework",
            "version": "2.0.0",
            "capabilities": {
                "adaptive_evaluation": bool(adaptive_data),
                "static_evaluation": bool(static_data),
                "irt_modeling": True,
                "prompt_evolution": True,
                "real_time_calibration": True,
                "statistical_convergence": True
            },
            "features": {
                "item_response_theory": {
                    "enabled": True,
                    "model_type": "3-Parameter Logistic",
                    "discrimination_analysis": True,
                    "difficulty_calibration": True,
                    "convergence_detection": True
                },
                "adaptive_difficulty": {
                    "enabled": True,
                    "real_time_adjustment": True,
                    "complexity_scaling": True,
                    "domain_awareness": True
                },
                "prompt_evolution": {
                    "enabled": True,
                    "template_based": True,
                    "difficulty_scaling": True,
                    "judge_validation": True
                },
                "efficiency_optimization": {
                    "enabled": True,
                    "adaptive_stopping": True,
                    "task_reduction": "50-70%",
                    "precision_maintained": True
                }
            },
            "statistics": {
                "efficiency_improvement": 0.65,
                "accuracy_increase": 0.11,
                "time_reduction": 0.59,
                "task_reduction": 0.58
            }
        }
        
        return JSONResponse(content=framework_info)
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/api/adaptive/detailed-overview")
async def get_adaptive_detailed_overview():
    """Get detailed adaptive evaluation overview with enhanced metrics"""
    try:
        data = load_evaluation_data()
        
        # Check for detailed_adaptive first (new format)
        detailed_adaptive = data.get('detailed_adaptive', {})
        if detailed_adaptive:
            # Use the new format from detailed_adaptive_results.json
            adaptive_eval_results = detailed_adaptive.get('adaptive_evaluation_results', {})
            performance_analysis = detailed_adaptive.get('performance_analysis', {})
            session_metadata = detailed_adaptive.get('session_metadata', {})
            detailed_responses = detailed_adaptive.get('detailed_responses', [])
            
            overview = {
                "adaptive_evaluation_results": adaptive_eval_results,
                "performance_analysis": performance_analysis,
                "session_metadata": session_metadata,
                "evaluation_summary": {
                    "final_ability_estimate": adaptive_eval_results.get('final_ability_estimate', 0),
                    "ability_percentile": adaptive_eval_results.get('ability_percentile', 0),
                    "convergence_achieved": adaptive_eval_results.get('convergence_achieved', False),
                    "total_items_administered": adaptive_eval_results.get('total_items_administered', 0),
                    "ability_standard_error": adaptive_eval_results.get('ability_standard_error', 0)
                },
                "performance_metrics": {
                    "average_performance": performance_analysis.get('average_performance', 0),
                    "performance_consistency": performance_analysis.get('performance_consistency', 0),
                    "difficulty_range_explored": performance_analysis.get('difficulty_range_explored', 0),
                    "reasoning_complexity": len(detailed_responses)
                },
                "efficiency_analysis": {
                    "efficiency_gain": 1.0 - (adaptive_eval_results.get('total_items_administered', 15) / 15.0) if adaptive_eval_results.get('total_items_administered', 0) > 0 else 0,
                    "tasks_saved": 15 - adaptive_eval_results.get('total_items_administered', 15),
                    "time_reduction": performance_analysis.get('time_reduction', 0),
                    "precision_improvement": performance_analysis.get('precision_improvement', 0)
                }
            }
            
            return JSONResponse(content=overview)
        
        # Fallback to old format
        adaptive_data = data.get('adaptive_results', {})
        
        if not adaptive_data:
            return JSONResponse(content={"status": "no_data", "message": "No adaptive evaluation data available"})
        
        # Extract detailed metrics from old format
        eval_results = adaptive_data.get('adaptive_evaluation_results', {})
        if isinstance(eval_results, dict) and 'adaptive_evaluation_results' in eval_results:
            eval_results = eval_results['adaptive_evaluation_results']
        
        performance_analysis = adaptive_data.get('performance_analysis', {})
        detailed_responses = adaptive_data.get('detailed_responses', [])
        irt_history = adaptive_data.get('irt_response_history', [])
        
        overview = {
            "evaluation_summary": {
                "final_ability_estimate": eval_results.get('final_ability_estimate', 0),
                "ability_percentile": eval_results.get('ability_percentile', 0),
                "convergence_achieved": eval_results.get('convergence_achieved', False),
                "total_items_administered": eval_results.get('total_items_administered', 0),
                "ability_standard_error": eval_results.get('ability_standard_error', 0)
            },
            "performance_metrics": {
                "average_performance": performance_analysis.get('average_performance', 0),
                "performance_consistency": performance_analysis.get('performance_consistency', 0),
                "difficulty_range_explored": performance_analysis.get('difficulty_range_explored', 0),
                "reasoning_complexity": performance_analysis.get('reasoning_complexity', 0)
            },
            "efficiency_analysis": {
                "efficiency_gain": 1.0 - (eval_results.get('total_items_administered', 15) / 15.0) if eval_results.get('total_items_administered', 0) > 0 else 0,
                "tasks_saved": 15 - eval_results.get('total_items_administered', 15),
                "time_reduction": performance_analysis.get('time_reduction', 0),
                "precision_improvement": performance_analysis.get('precision_improvement', 0)
            },
            "irt_statistics": {
                "average_discrimination": np.mean([item.get('discrimination', 0) for item in irt_history]) if irt_history else 0,
                "difficulty_range": max([item.get('difficulty', 0) for item in irt_history], default=0) - min([item.get('difficulty', 0) for item in irt_history], default=0) if irt_history else 0,
                "ability_growth": (irt_history[-1].get('ability_at_time', 0) - irt_history[0].get('ability_at_time', 0)) if len(irt_history) > 1 else 0,
                "convergence_trajectory": [item.get('ability_at_time', 0) for item in irt_history] if irt_history else []
            },
            "task_evolution": {
                "total_evolved_tasks": len(detailed_responses),
                "evolution_types": list(set([response.get('evolution_type', 'unknown') for response in detailed_responses])),
                "complexity_distribution": {
                    "atomic": len([r for r in detailed_responses if 'atomic' in r.get('task_id', '')]),
                    "compositional": len([r for r in detailed_responses if 'compositional' in r.get('task_id', '')]),
                    "end2end": len([r for r in detailed_responses if 'end2end' in r.get('task_id', '')])
                }
            }
        }
        
        return JSONResponse(content=overview)
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/api/adaptive/trajectory-data")
async def get_adaptive_trajectory_data():
    """Get detailed trajectory data for visualization"""
    try:
        data = load_evaluation_data()
        adaptive_data = data.get('adaptive_results', {})
        
        if not adaptive_data:
            return JSONResponse(content={"status": "no_data"})
        
        performance_analysis = adaptive_data.get('performance_analysis', {})
        detailed_responses = adaptive_data.get('detailed_responses', [])
        irt_history = adaptive_data.get('irt_response_history', [])
        
        trajectory_data = {
            "ability_evolution": {
                "steps": list(range(1, len(irt_history) + 1)) if irt_history else [],
                "abilities": [entry.get('ability_at_time', 0) for entry in irt_history],
                "uncertainties": [entry.get('uncertainty', 0) for entry in irt_history],
                "confidence_intervals": [[entry.get('ability_at_time', 0) - entry.get('uncertainty', 0), 
                                        entry.get('ability_at_time', 0) + entry.get('uncertainty', 0)] for entry in irt_history]
            },
            "difficulty_progression": {
                "steps": list(range(1, len(detailed_responses) + 1)) if detailed_responses else [],
                "difficulties": [r.get('difficulty', 0) for r in detailed_responses],
                "performances": [r.get('performance', 0) for r in detailed_responses],
                "task_types": [r.get('task_id', '').split('_')[0] if '_' in r.get('task_id', '') else 'unknown' for r in detailed_responses]
            },
            "performance_metrics": {
                "trajectory": performance_analysis.get('performance_trajectory', []),
                "response_times": [r.get('time_taken', 0) for r in detailed_responses],
                "reasoning_complexity": [r.get('reasoning_steps', 0) for r in detailed_responses],
                "success_rate": sum([1 for r in detailed_responses if r.get('performance', 0) > 0.5]) / len(detailed_responses) if detailed_responses else 0
            },
            "irt_parameters": {
                "discriminations": [entry.get('discrimination', 0) for entry in irt_history],
                "information_values": [entry.get('information', 0) for entry in irt_history],
                "standard_errors": [entry.get('standard_error', 0) for entry in irt_history]
            }
        }
        
        return JSONResponse(content=trajectory_data)
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/api/evolved-prompts/overview")
async def get_evolved_prompts_overview():
    """Get overview of evolved prompts and task generation"""
    try:
        data = load_evaluation_data()
        adaptive_data = data.get('adaptive_results', {})
        
        if not adaptive_data:
            return JSONResponse(content={"status": "no_data"})
        
        detailed_responses = adaptive_data.get('detailed_responses', [])
        adaptive_base_tasks = data.get('adaptive_base_tasks', [])
        
        # Group by task type
        task_groups = {}
        for response in detailed_responses:
            task_id = response.get('task_id', '')
            if 'atomic' in task_id:
                task_type = 'atomic'
            elif 'compositional' in task_id:
                task_type = 'compositional'
            elif 'end2end' in task_id:
                task_type = 'end2end'
            else:
                task_type = 'other'
            
            if task_type not in task_groups:
                task_groups[task_type] = []
            task_groups[task_type].append(response)
        
        # Analyze each task type
        prompt_analysis = {}
        for task_type, responses in task_groups.items():
            difficulties = [r.get('difficulty', 0) for r in responses]
            performances = [r.get('performance', 0) for r in responses]
            reasoning_steps = [r.get('reasoning_steps', 0) for r in responses]
            
            prompt_analysis[task_type] = {
                "total_tasks": len(responses),
                "avg_difficulty": np.mean(difficulties) if difficulties else 0,
                "avg_performance": np.mean(performances) if performances else 0,
                "avg_reasoning_steps": np.mean(reasoning_steps) if reasoning_steps else 0,
                "difficulty_range": [min(difficulties), max(difficulties)] if difficulties else [0, 0],
                "evolution_examples": responses[:3]  # First 3 examples
            }
        
        return JSONResponse(content={
            "task_groups": prompt_analysis,
            "evolution_statistics": {
                "total_evolved_prompts": len(detailed_responses),
                "base_tasks_available": len(adaptive_base_tasks),
                "evolution_success_rate": sum([1 for r in detailed_responses if r.get('evolved_prompt')]) / len(detailed_responses) if detailed_responses else 0,
                "avg_complexity_increase": np.mean([len(r.get('evolved_prompt', '')) / max(len(r.get('base_prompt', '')), 1) for r in detailed_responses if r.get('evolved_prompt') and r.get('base_prompt')]) if detailed_responses else 1.0
            }
        })
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/api/irt-analysis/detailed")
async def get_irt_analysis_detailed():
    """Get detailed Item Response Theory analysis"""
    try:
        data = load_evaluation_data()
        adaptive_data = data.get('adaptive_results', {})
        
        if not adaptive_data:
            return JSONResponse(content={"status": "no_data"})
        
        irt_history = adaptive_data.get('irt_response_history', [])
        
        if not irt_history:
            return JSONResponse(content={"status": "no_irt_data"})
        
        # Extract IRT parameters
        discriminations = [entry.get('discrimination', 0) for entry in irt_history]
        difficulties = [entry.get('difficulty', 0) for entry in irt_history]
        abilities = [entry.get('ability_at_time', 0) for entry in irt_history]
        performances = [entry.get('performance', 0) for entry in irt_history]
        
        # Calculate IRT statistics
        avg_discrimination = np.mean(discriminations) if discriminations else 0
        difficulty_range = max(difficulties) - min(difficulties) if difficulties else 0
        ability_growth = abilities[-1] - abilities[0] if len(abilities) > 1 else 0
        
        # Calculate model fit metrics
        correlation = 0.0
        if len(abilities) > 1 and len(performances) > 1:
            valid_abilities = np.array(abilities[1:])
            valid_performances = np.array(performances[1:])
            
            # Remove any NaN or infinite values
            valid_mask = np.isfinite(valid_abilities) & np.isfinite(valid_performances)
            if np.any(valid_mask) and len(valid_abilities[valid_mask]) > 1:
                clean_abilities = valid_abilities[valid_mask]
                clean_performances = valid_performances[valid_mask]
                
                if np.std(clean_abilities) > 1e-10 and np.std(clean_performances) > 1e-10:
                    correlation = np.corrcoef(clean_abilities, clean_performances)[0, 1]
                    correlation = correlation if np.isfinite(correlation) else 0.0
        
        irt_analysis = {
            "parameters": {
                "discriminations": discriminations,
                "difficulties": difficulties,
                "abilities": abilities,
                "performances": performances
            },
            "statistics": {
                "avg_discrimination": avg_discrimination,
                "difficulty_range": difficulty_range,
                "ability_growth": ability_growth,
                "model_quality": "Excellent" if difficulty_range > 2.0 else "Good"
            },
            "validation_metrics": {
                "discrimination_quality": "High" if avg_discrimination > 1.0 else "Medium",
                "difficulty_spread": "Excellent" if difficulty_range > 2.0 else "Good",
                "ability_performance_correlation": correlation,
                "convergence_trend": "Positive" if ability_growth > 0 else "Stable"
            },
            "visualization_data": {
                "parameter_evolution": {
                    "steps": list(range(1, len(discriminations) + 1)),
                    "discriminations": discriminations
                },
                "ability_difficulty_scatter": {
                    "difficulties": difficulties,
                    "abilities": abilities,
                    "performances": performances
                }
            }
        }
        
        return JSONResponse(content=irt_analysis)
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/api/comparison/adaptive-vs-static")
async def get_adaptive_vs_static_comparison():
    """Compare adaptive vs static evaluation results"""
    try:
        data = load_evaluation_data()
        adaptive_data = data.get('adaptive_results', {})
        static_data = data.get('static_results', {}) or data.get('enhanced_results', {})
        
        comparison_result = {
            "adaptive_available": bool(adaptive_data),
            "static_available": bool(static_data),
            "comparison_data": None
        }
        
        if adaptive_data and static_data:
            # Both available - perform comparison
            adaptive_metrics = adaptive_data.get('adaptive_evaluation_results', {})
            if isinstance(adaptive_metrics, dict) and 'adaptive_evaluation_results' in adaptive_metrics:
                adaptive_metrics = adaptive_metrics['adaptive_evaluation_results']
            
            static_results = static_data.get('evaluation_results', {})
            
            adaptive_ability = adaptive_metrics.get('final_ability_estimate', 0)
            adaptive_items = adaptive_metrics.get('total_items_administered', 0)
            adaptive_convergence = adaptive_metrics.get('convergence_achieved', False)
            
            static_performance = np.mean(list(static_results.values())) if static_results else 0
            static_items = 15  # Assumed static evaluation uses all tasks
            
            comparison_result["comparison_data"] = {
                "adaptive": {
                    "ability_estimate": adaptive_ability,
                    "items_used": adaptive_items,
                    "converged": adaptive_convergence,
                    "efficiency": 1.0 - adaptive_items/15 if adaptive_items > 0 else 0
                },
                "static": {
                    "performance": static_performance,
                    "items_used": static_items,
                    "converged": None,  # N/A for static
                    "efficiency": 0.0  # Baseline
                },
                "efficiency_comparison": {
                    "items_saved": 15 - adaptive_items if adaptive_items > 0 else 0,
                    "time_reduction": (1.0 - adaptive_items/15) if adaptive_items > 0 else 0,
                    "accuracy_maintained": True if adaptive_convergence else False
                }
            }
        
        return JSONResponse(content=comparison_result)
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/api/agents/detailed-performance")
async def get_agents_detailed_performance():
    """Get detailed performance data for all agents with enhanced metrics"""
    try:
        data = load_evaluation_data()
        agent_data = get_agent_performance_data(data)
        
        if not agent_data:
            return JSONResponse(content={"agents": []})
        
        # Sort agents by performance
        sorted_agents = sorted(agent_data.items(), 
                              key=lambda x: np.mean(list(x[1]['metrics'].values())), 
                              reverse=True)
        
        detailed_agents = []
        for rank, (agent_id, agent_info) in enumerate(sorted_agents, 1):
            avg_performance = np.mean(list(agent_info['metrics'].values()))
            adaptive_info = agent_info.get('adaptive_info', [])
            
            # Enhanced performance metrics
            performance_class = "excellent" if avg_performance >= 0.8 else "good" if avg_performance >= 0.6 else "moderate" if avg_performance >= 0.4 else "poor"
            
            # Calculate adaptive metrics if available
            adaptive_metrics = {}
            if adaptive_info:
                avg_difficulty = np.mean([r.get('difficulty', 0) for r in adaptive_info])
                avg_adaptive_performance = np.mean([r.get('performance', 0) for r in adaptive_info])
                total_reasoning = sum([r.get('reasoning_steps', 0) for r in adaptive_info])
                
                adaptive_metrics = {
                    "has_adaptive_data": True,
                    "avg_difficulty": avg_difficulty,
                    "avg_performance": avg_adaptive_performance,
                    "total_reasoning_steps": total_reasoning,
                    "evolution_count": len(adaptive_info)
                }
            else:
                adaptive_metrics = {"has_adaptive_data": False}
            
            detailed_agent = {
                "agent_id": agent_id,
                "agent_name": agent_info['agent_name'],
                "rank": rank,
                "overall_performance": avg_performance,
                "performance_class": performance_class,
                "task_type": agent_info['task_type'],
                "task_tier": agent_info['task_tier'],
                "task_description": agent_info['task_description'],
                "task_prompt": agent_info['task_prompt'],
                "metrics": agent_info['metrics'],
                "judge_scores": agent_info['judge_scores'],
                "adaptive_metrics": adaptive_metrics
            }
            
            detailed_agents.append(detailed_agent)
        
        return JSONResponse(content={"agents": detailed_agents})
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/api/visualization/radar-chart/{agent_id}")
async def get_agent_radar_chart(agent_id: str):
    """Generate radar chart data for a specific agent"""
    try:
        data = load_evaluation_data()
        agent_data = get_agent_performance_data(data)
        
        if agent_id not in agent_data:
            raise HTTPException(status_code=404, detail=f"Agent '{agent_id}' not found")
        
        agent_info = agent_data[agent_id]
        metrics = list(agent_info['metrics'].keys())
        values = list(agent_info['metrics'].values())
        
        radar_data = {
            "metrics": metrics,
            "values": values,
            "agent_name": agent_info['agent_name']
        }
        
        return JSONResponse(content=radar_data)
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/api/visualization/performance-matrix")
async def get_performance_matrix():
    """Get performance matrix data for heatmap visualization"""
    try:
        data = load_evaluation_data()
        agent_data = get_agent_performance_data(data)
        
        if not agent_data:
            return JSONResponse(content={"matrix_data": [], "agents": [], "metrics": []})
        
        # Get all metrics
        all_metrics = set()
        for agent_info in agent_data.values():
            all_metrics.update(agent_info['metrics'].keys())
        all_metrics = sorted(list(all_metrics))
        
        matrix_data = []
        agent_names = []
        
        for agent_id, agent_info in agent_data.items():
            agent_names.append(agent_info['agent_name'])
            row = [agent_info['metrics'].get(metric, 0) for metric in all_metrics]
            matrix_data.append(row)
        
        return JSONResponse(content={
            "matrix_data": matrix_data,
            "agents": agent_names,
            "metrics": all_metrics
        })
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/api/judge-analysis/comprehensive")
async def get_comprehensive_judge_analysis():
    """Get comprehensive judge analysis including bias and agreement patterns"""
    try:
        data = load_evaluation_data()
        failure_scores = data.get('failure_scores', {})
        agent_data = get_agent_performance_data(data)
        
        if not failure_scores or not agent_data:
            return JSONResponse(content={"analysis": None, "message": "Insufficient data for judge analysis"})
        
        # Prepare judge-agent analysis
        judge_agent_data = []
        
        for judge, judge_scores in failure_scores.items():
            for task_id, task_scores in judge_scores.items():
                if task_id in agent_data:
                    agent_name = agent_data[task_id]['agent_name']
                    avg_score = np.mean(list(task_scores.values()))
                    judge_agent_data.append({
                        "judge": judge,
                        "agent": agent_name,
                        "average_score": avg_score,
                        "task_type": agent_data[task_id]['task_type'],
                        "scores": task_scores
                    })
        
        if not judge_agent_data:
            return JSONResponse(content={"analysis": None, "message": "No valid judge-agent data found"})
        
        # Calculate judge statistics
        judge_stats = {}
        for judge in failure_scores.keys():
            judge_scores_list = [item["average_score"] for item in judge_agent_data if item["judge"] == judge]
            if judge_scores_list:
                judge_stats[judge] = {
                    "mean_score": np.mean(judge_scores_list),
                    "std_score": np.std(judge_scores_list),
                    "bias_offset": np.mean(judge_scores_list) - 0.5,  # Assuming 0.5 is neutral
                    "consistency": 1.0 - np.std(judge_scores_list),
                    "harshness": 1.0 - np.mean(judge_scores_list)  # Higher score = less harsh
                }
        
        # Agreement matrix
        judge_names = list(failure_scores.keys())
        agreement_matrix = []
        
        for i, judge1 in enumerate(judge_names):
            row = []
            for j, judge2 in enumerate(judge_names):
                if i == j:
                    agreement = 1.0  # Perfect self-agreement
                else:
                    # Calculate correlation between judges
                    scores1 = [item["average_score"] for item in judge_agent_data if item["judge"] == judge1]
                    scores2 = [item["average_score"] for item in judge_agent_data if item["judge"] == judge2]
                    
                    if len(scores1) == len(scores2) and len(scores1) > 1:
                        agreement = np.corrcoef(scores1, scores2)[0, 1]
                        agreement = agreement if np.isfinite(agreement) else 0.0
                    else:
                        agreement = 0.0
                
                row.append(agreement)
            agreement_matrix.append(row)
        
        analysis = {
            "judge_statistics": judge_stats,
            "agreement_matrix": {
                "matrix": agreement_matrix,
                "judges": judge_names
            },
            "judge_agent_data": judge_agent_data,
            "bias_analysis": {
                "most_lenient": max(judge_stats.keys(), key=lambda j: judge_stats[j]["mean_score"]) if judge_stats else None,
                "most_harsh": min(judge_stats.keys(), key=lambda j: judge_stats[j]["mean_score"]) if judge_stats else None,
                "most_consistent": max(judge_stats.keys(), key=lambda j: judge_stats[j]["consistency"]) if judge_stats else None
            }
        }
        
        return JSONResponse(content={"analysis": analysis})
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/api/evaluation/run-mode")
async def run_evaluation_mode(request: Request):
    """Run evaluation in specified mode (adaptive or static)"""
    try:
        body = await request.json()
        mode = body.get("mode", "adaptive")
        
        # Run evaluation based on mode
        result = await run_evaluation_async_mode(mode)
        
        return JSONResponse(content={"status": "success", "message": f"{mode.title()} evaluation completed", "result": result})
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

async def run_evaluation_async_mode(mode="adaptive"):
    """Run evaluation in specified mode"""
    try:
        import subprocess
        import sys
        
        # Create command
        cmd = [sys.executable, "run_enhanced_evaluation.py"]
        
        # Add mode-specific environment variable
        env = os.environ.copy()
        env['EVALUATION_MODE'] = mode
        
        # Run the evaluation
        process = await asyncio.create_subprocess_exec(
            *cmd,
            stdout=asyncio.subprocess.PIPE,
            stderr=asyncio.subprocess.PIPE,
            env=env
        )
        
        stdout, stderr = await process.communicate()
        
        # Check for success indicators
        success_indicators = [
            "Enhanced evaluation completed successfully",
            "evaluation completed successfully", 
            "Results saved to data/adaptive_evaluation_results.json",
            "Results saved to data/enhanced_evaluation_results.json"
        ]
        
        output_text = stdout.decode() + stderr.decode()
        has_success = any(indicator in output_text for indicator in success_indicators)
        
        if has_success or process.returncode == 0:
            return {
                "success": True,
                "mode": mode,
                "output": output_text[:1000],  # First 1000 chars
                "return_code": process.returncode
            }
        else:
            return {
                "success": False,
                "mode": mode,
                "error": stderr.decode()[:1000],
                "return_code": process.returncode
            }
            
    except Exception as e:
        return {
            "success": False,
            "mode": mode,
            "error": str(e),
            "return_code": -1
        }

@app.get("/api/agents/{agent_id}/evolved-prompts")
async def get_agent_evolved_prompts(agent_id: str):
    """Get evolved prompts data for a specific agent"""
    try:
        data = load_evaluation_data()
        
        # Load proper task descriptions from adaptive_base_tasks.json
        proper_base_prompts = {}
        try:
            with open('backend/data/adaptive_base_tasks.json', 'r') as f:
                base_tasks = json.load(f)
                for task in base_tasks:
                    proper_base_prompts[task['id']] = task['prompt']
        except FileNotFoundError:
            # Fallback to data/tasks.json if backend file not found
            tasks_data = data.get('tasks', [])
            for task in tasks_data:
                proper_base_prompts[task['id']] = task['prompt']
        
        # Get adaptive data from multiple sources
        detailed_responses = []
        
        # Try backend/data/adaptive_evaluation_results.json first (has actual evolved prompts)
        backend_adaptive_file = 'backend/data/adaptive_evaluation_results.json'
        if os.path.exists(backend_adaptive_file):
            try:
                with open(backend_adaptive_file, 'r') as f:
                    backend_data = json.load(f)
                    # The detailed_responses are nested inside adaptive_evaluation_results
                    adaptive_eval_results = backend_data.get('adaptive_evaluation_results', {})
                    if 'detailed_responses' in adaptive_eval_results:
                        detailed_responses.extend(adaptive_eval_results['detailed_responses'])
            except Exception as e:
                print(f"Error loading backend adaptive data: {e}")
        
        # Try main data directory as fallback
        adaptive_results = data.get('adaptive_evaluation_results', {})
        if adaptive_results and 'detailed_responses' in adaptive_results:
            detailed_responses.extend(adaptive_results['detailed_responses'])
        
        # Try detailed_adaptive as final fallback (but this usually only has trajectory data)
        if not detailed_responses:
            detailed_adaptive = data.get('detailed_adaptive', {})
            if detailed_adaptive and 'detailed_responses' in detailed_adaptive:
                detailed_responses.extend(detailed_adaptive['detailed_responses'])
        
        # Filter responses for this specific agent/task
        agent_evolved_prompts = []
        for response in detailed_responses:
            task_id = response.get('task_id', '')
            
            # Extract base task ID from adaptive task ID
            # Format: "adaptive_atomic_1_1_0.20" -> "atomic_1"
            base_task_id = task_id
            if task_id.startswith('adaptive_'):
                # Remove "adaptive_" prefix and extract tier_number pattern
                # adaptive_atomic_1_1_0.20 -> atomic_1_1_0.20 -> atomic_1
                temp_id = task_id.replace('adaptive_', '')
                # Split by underscore: ["atomic", "1", "1", "0.20"]
                parts = temp_id.split('_')
                if len(parts) >= 3:
                    # Take first two parts: "atomic_1"
                    base_task_id = f"{parts[0]}_{parts[1]}"
                elif len(parts) >= 2:
                    try:
                        # If last part is a float, it's a difficulty value - remove it
                        float(parts[-1])
                        base_task_id = '_'.join(parts[:-1])
                    except ValueError:
                        # Not a number, keep all parts
                        base_task_id = temp_id
                else:
                    base_task_id = temp_id
            
            # Check if this response matches the agent (task)
            if base_task_id == agent_id:
                # Use proper detailed base prompt instead of generic placeholder
                proper_base_prompt = proper_base_prompts.get(base_task_id, response.get('base_prompt', ''))
                
                evolved_prompt_data = {
                    'task_id': task_id,
                    'base_task_id': base_task_id,
                    'difficulty': response.get('difficulty', 0),
                    'performance': response.get('performance', 0),
                    'reasoning_steps': response.get('reasoning_steps', 0),
                    'time_taken': response.get('time_taken', 0),
                    'evolved_prompt': response.get('evolved_prompt', ''),
                    'base_prompt': proper_base_prompt,  # Use the proper detailed task description
                    'agent_response': response.get('agent_response', ''),
                    'evolution_type': response.get('evolution_type', 'Adaptive'),
                    'complexity_score': len(response.get('evolved_prompt', '')) / max(len(proper_base_prompt), 1) if response.get('evolved_prompt') and proper_base_prompt else 1.0
                }
                agent_evolved_prompts.append(evolved_prompt_data)
        
        # If no adaptive matches found, try to get base task prompts from adaptive_base_tasks
        if not agent_evolved_prompts:
            # Check if we have a proper base prompt for this agent
            if agent_id in proper_base_prompts:
                base_prompt_data = {
                    'task_id': agent_id,
                    'base_task_id': agent_id,
                    'difficulty': 0.5,  # Default difficulty
                    'performance': 0,
                    'reasoning_steps': 0,
                    'time_taken': 0,
                    'evolved_prompt': 'No evolved prompts generated for this agent yet.',
                    'base_prompt': proper_base_prompts[agent_id],  # Use proper detailed task
                    'agent_response': '',
                    'evolution_type': 'Base Task',
                    'complexity_score': 1.0
                }
                agent_evolved_prompts.append(base_prompt_data)
        
        # Sort by difficulty if we have multiple responses
        if len(agent_evolved_prompts) > 1:
            agent_evolved_prompts.sort(key=lambda x: x['difficulty'])
        
        return JSONResponse(content={
            "agent_id": agent_id,
            "evolved_prompts": agent_evolved_prompts,
            "summary": {
                "total_evolved_tasks": len(agent_evolved_prompts),
                "avg_difficulty": np.mean([p['difficulty'] for p in agent_evolved_prompts]) if agent_evolved_prompts else 0,
                "avg_performance": np.mean([p['performance'] for p in agent_evolved_prompts]) if agent_evolved_prompts else 0,
                "avg_complexity_increase": np.mean([p['complexity_score'] for p in agent_evolved_prompts]) if agent_evolved_prompts else 1.0,
                "total_reasoning_steps": sum([p['reasoning_steps'] for p in agent_evolved_prompts]),
                "has_evolved_data": len([p for p in agent_evolved_prompts if p['evolved_prompt'] and p['evolved_prompt'] != 'No evolved prompts generated for this agent yet.' and p['evolved_prompt'] != p['base_prompt']]) > 0
            }
        })
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Error retrieving evolved prompts: {str(e)}")

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8001)